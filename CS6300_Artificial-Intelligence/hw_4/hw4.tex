\documentclass[12pt]{article}

\usepackage{times}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{automata, arrows, positioning, calc}
\usepackage{color}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.9in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\topmargin}{0.05in}
\setlength{\headheight}{-0.05in}
\setlength{\headsep}{0.0in}

\begin{document}

\begin{center}
{\bf CS 6300} \hfill {\large\bf HW4: Value Iteration} \hfill {\bf Due February 21, 2017}
\end{center}

\noindent
Please use the \LaTeX\ template to produce your writeups. See the
Homework Assignments page on the class website for details.  Hand in
at: \url{https://webhandin.eng.utah.edu/index.php}.

\section{Value Iteration}

At the AI casino, there are two things to do: Eat Buffet and Play AI
Blackjack.  You start out Poor and Hungry, and would like to leave the
casino Rich and Full.  If you Play while you are Full you are more
likely to become Rich, but if you are Poor you may have a hard time
becoming Full on your budget.  We can model your decision making
process as the following MDP:

\begin{flushleft}
\begin{tabular}{ll}
State Space & \{PoorHungry, PoorFull, RichHungry, RichFull\} \\
Actions     & \{Eat, Play\} \\
Initial State &  PoorHungry \\
Terminal State & RichFull 
\end{tabular}
\end{flushleft}

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{|l|l|l|l|} \hline
$s$        & $a$  & $s'$       & $T(s,a,s')$ \\ \hline
PoorHungry & Play & PoorHungry & 0.8 \\ \hline
PoorHungry & Play & RichHungry & 0.2 \\ \hline
PoorHungry & Eat  & PoorHungry & 0.8 \\ \hline
PoorHungry & Eat  & PoorFull   & 0.2 \\ \hline
PoorFull   & Play & PoorFull   & 0.5 \\ \hline
PoorFull   & Play & RichFull   & 0.5 \\ \hline
RichHungry & Eat  & RichHungry & 0.2 \\ \hline
RichHungry & Eat  &RichFull    & 0.8 \\ \hline
\end{tabular} &
\begin{tabular}{|l|l|}\hline
$s'$       & $R(s')$ \\ \hline
PoorHungry & -1 \\  \hline
PoorFull   &  1 \\ \hline
RichHungry &  0 \\ \hline
RichFull   &  5 \\ \hline
\end{tabular} \\
Transition Model & Rewards
\end{tabular}
\end{center}

\begin{center}
\begin{tikzpicture}[->, >=stealth', auto, semithick, node distance=3cm]
\tikzstyle{every state}=[fill=white,draw=black,thick,text=black,scale=1]
\node[state]    (PH)                     {$PH$};
\node[state]    (PF)[above right of=PH]   {$PF$};
\node[state]    (RH)[below right of=PH]   {$RH$};
\node[state]    (RF)[below right of=PF]   {$RF$};
\path
(PH) edge[loop left, red]     node{$0.8$}         (PH)
     edge[loop right, blue]   node{$0.8$}         (PH)
     edge[bend left, red]     node{$0.2$}         (PF)
     edge[bend right, blue]     node[below left]{$0.2$}       (RH)
(PF) edge[loop below, blue]    node{$0.5$}         (PF)
     edge[bend left, blue]    node{$0.5$}        (RF)
(RH) edge[loop above, red]    node{$0.2$}        (RH)
     edge[bend right, red]     node[below right]{$0.8$}        (RF);
\end{tikzpicture}

Where {\color{red}{red}} denotes the action to {\color{red}{Eat}} and {\color{blue}{blue}} denotes {\color{blue}{Play}}.
\end{center}

\newpage
\begin{enumerate}

\item Complete the table for the first 3 iterations of Value
  Iteration. Assume $\gamma = 1$.

\begin{center}
\begin{tabular}{|l|c|c|c|c|} \hline
State      & $i=0$ & $i=1$ & $i=2$ & $i=3$ \\ \hline \hline
PoorHungry &$\phantom{-}0.00$&$-1.00$&$-1.60$&$-1.98$\\ \hline
PoorFull   &$\phantom{-}0.00$&$\phantom{-}1.00$&$\phantom{-}1.50$&$\phantom{-}1.75$\\ \hline
RichHungry &$\phantom{-}0.00$&$\phantom{-}0.00$&$\phantom{-}0.00$&$\phantom{-}0.00$\\ \hline
RichFull   &$\phantom{-}0.00$&$\phantom{-}0.00$&$\phantom{-}0.00$&$\phantom{-}0.00$\\ \hline
\end{tabular}
\end{center}

The values of {\em PoorHungry} ($PH$) is calculated for two iterations to show the steps.

\begin{align*}
 V_{1}(PH) &= R(PH) + \gamma\max_{a}\left\{\begin{array}{l}T(PH,\ play,\ PH)V_{0}(PH) + T(PH,\ play,\ RH)V_{0}(RH)\\ T(PH,\ eat,\ PH)V_{0}(PH) + T(PH,\ eat,\ PF)V_{0}(PF)\\\end{array}\right.\\
V_{1}(PH) &= -1.0 + (\gamma = 1)\max_{a}\left\{\begin{array}{l}0.8(-1) + 0.2(0)\\ 0.8(-1) + 0.2(0)\\\end{array}\right.\\
V_{1}(PH) &= -1.0
\end{align*}

\begin{align*}
 V_{2}(PH) &= R(PH) + \gamma\max_{a}\left\{\begin{array}{l}T(PH,\ play,\ PH)V_{1}(PH) + T(PH,\ play,\ RH)V_{1}(RH)\\ T(PH,\ eat,\ PH)V_{1}(PH) + T(PH,\ eat,\ PF)V_{1}(PF)\\\end{array}\right.\\
V_{2}(PH) &= -1.0 + (\gamma = 1)\max_{a}\left\{\begin{array}{l}0.8(-1) + 0.2(0)\\ 0.8(-1) + 0.2(1.0)\\\end{array}\right.\\
V_{2}(PH) &= -1.6
\end{align*}

\item Assuming that we are acting for three time steps, what is the
  optimal action to take from the starting state? Justify your answer.

The convergence of the system is $PH = -3.00$, $PF = 2.00$, and $RH = 0.00$, so the optimal strategy would be to eat first as it's the highest value. The same is true if we just consider 3 iterations, however the values are different but the resulting importance is the same.
\end{enumerate}

\end{document}
